{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a follow up on my previous kernel about the structure in missing values and whether one can make use of it. You can find the previous kernel [here](https://www.kaggle.com/cgump3rt/two-sigma-financial-modeling/investigate-missing-values). The short summary from the training data is:\n",
    "\n",
    "* fraction of missing data can is significant for many feature columns,\n",
    "* NaNs are **not** distributed randomly per feature but span a continuous range in time\n",
    "* all NaNs for an asset ID appear at the beginning of the period during which this asset is part of the portfolio.\n",
    "\n",
    "I saw that there are already quite a few interesting kernels by other people who also tried to cluster assets based on the NaN structure. For example have a look at the following kernels if you are interested (list non-exhaustive)\n",
    "\n",
    "* by [jrisk123](https://www.kaggle.com/jriskas/two-sigma-financial-modeling/asset-cluster-visualization)\n",
    "* by [lesibius](https://www.kaggle.com/lesibius/two-sigma-financial-modeling/financial-instrument-types)\n",
    "* by [chbimo](https://www.kaggle.com/chbimo/two-sigma-financial-modeling/correlation-of-nan-structure-between-id)\n",
    "* by [Allunia](https://www.kaggle.com/allunia/two-sigma-financial-modeling/feature-dynamics-looking-at-id-groups)\n",
    "* by [reziproke](https://www.kaggle.com/reziproke/two-sigma-financial-modeling/nan-structure-on-id-level-eda)\n",
    "* (sorry if I missed you, add comment and I will update the list...)\n",
    "\n",
    "Before giving the clusterization yet another try, something came to my mind when I was skiing last week. What if data is published in fixed intervals (e.g. every working day at 9am, or Mondays, or the first Tuesday of the month)? In this scenario, the NaN structure does not necessarily characterize different asset categories, but also depends when an asset was bought relative to the next _publication date_. Imagine data were to be published always on Mondays. Then the number of missing NaN values would probably tell you which weekday you bough this asset.\n",
    "\n",
    "Long story short: let's see whether we can find some pattern in the total number of NaN values per timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with pd.HDFStore('train.h5') as train:\n",
    "    df = train.get('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we get a new data frame containing the total number of NaN values for each column and timestamp. Since we are not that much interested in the total numbers, but rather in how they change, we take the difference between two subsequent timestamps. Finally, we make a plot showing the evolution of the total number of NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get total number of NaN values per feature column for every timestamp\n",
    "nans = df.groupby('timestamp').apply(lambda x: x.isnull().sum())\n",
    "# get change with respect \n",
    "nans = nans.diff().drop(['id','timestamp','y'],axis=1)\n",
    "nans.plot(figsize=(24,12))\n",
    "plt.legend(ncol=4,loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila, at a first glance, it looks that data becomes available in regular intervals. It is interesting to see that there huge spikes in the beginning while later on, the magnitude becomes much smaller. But please not that these are absolute numbers. So the total number of assets in the portfolio is important. My guess is that we start with a large number of assets at timestamp 0. Therefore, we have a lot of NaNs which get filled over time. Over the training period, only comparably small chunks of assets are added leading to a smaller number of NaNs which gets filled later on.  \n",
    "There are also few timestamps at which the number of NaNs in the dataset increases. My suspicion is that these correspond to times where new assets are bought. So let's check this quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the number of assets per timestamp\n",
    "nassets = df.groupby('timestamp').apply(len)\n",
    "# get the change with respect to the previous timestamp\n",
    "delta_assets = nassets.diff()\n",
    "\n",
    "#delta_assets.plot(style=['.b'],ax=plt.gca())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids = sorted(df.id.unique())\n",
    "columns = df.columns.drop(['id','timestamp','y']).insert(0,'length')\n",
    "nan_df = pd.DataFrame(data=None,index=ids,columns=columns,dtype=float)\n",
    "# iterate over all asset ID\n",
    "for name,group in df.groupby('id'):\n",
    "    # for every feature column\n",
    "    for c in columns:\n",
    "        if c == 'length':\n",
    "            nan_df.loc[name,c] = int(len(group))\n",
    "        else:\n",
    "            # total number of rows with missing data\n",
    "            nan_df.loc[name,c] = float(group[c].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nan_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binary = nan_df.drop(['length'],axis=1)\n",
    "binary[binary > 0] = 1\n",
    "binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import dbscan\n",
    "_,labels = dbscan(binary.values)\n",
    "plt.hist(labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fractional = nan_df.div(nan_df['length'],axis='index').drop(['length'],axis=1)\n",
    "fractional.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_,labels = dbscan(fractional.values)\n",
    "plt.hist(labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fractional2 = nan_df.copy()\n",
    "fractional2[fractional2 > 200] = 200\n",
    "fractional2 = fractional2.div(fractional2['length'],axis='index').drop(['length'],axis=1)\n",
    "fractional2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_,labels = dbscan(fractional2.values)\n",
    "plt.hist(labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a2 = a\n",
    "nassets = df.groupby('timestamp').apply(len)\n",
    "a2['fundamental'] = a.filter(regex='fundamental_*').sum(axis=1)/nassets\n",
    "a2['derived'] = a.filter(regex='derived_*').sum(axis=1)/nassets\n",
    "a2['technical'] = a.filter(regex='technical_*').sum(axis=1)/nassets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a2[['technical']].plot(figsize=(24,12))\n",
    "#(0.25*b).plot(style=['r.'],ax=plt.gca())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f,((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2,figsize=(30,15))\n",
    "a[50:250].plot(ax=ax1,legend=False)\n",
    "b[50:250].plot(style=['.b'],ax=ax1)\n",
    "a[250:500].plot(ax=ax2,legend=False)\n",
    "b[250:500].plot(style=['.b'],ax=ax2)\n",
    "a[500:750].plot(ax=ax3,legend=False)\n",
    "b[500:750].plot(style=['.b'],ax=ax3)\n",
    "a[750:1000].plot(ax=ax4,legend=False)\n",
    "b[750:1000].plot(style=['.b'],ax=ax4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_lag = 250\n",
    "corrs = np.zeros((max_lag,a.shape[1]))\n",
    "for l in range(1,max_lag):\n",
    "    c = a.shift(-l)\n",
    "    corrs[l] = c.corrwith(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,18))\n",
    "sns.heatmap(corrs,xticklabels=a.columns.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
